/*
 * networkDummy_Float.c
 *
 *  Created on: Jul 24, 2025
 *      Author: lukasschoepf
 */
/*
 * network_16_float.c
 *
 *  Created on: Jul 25, 2025
 *      Author: lukasschoepf
 */
/* AUTOGENERATED DO NOT MODIFY */

/**
  ******************************************************************************
  * @file    network.c
  * @brief   NN Code autogenerated DO NOT MODIFY IT
  ******************************************************************************
  * @attention
  *
  * Copyright (c) 2023 STMicroelectronics.
  * All rights reserved.
  *
  * This software is licensed under terms that can be found in the LICENSE file
  * in the root directory of this software component.
  * If no LICENSE file comes with this software, it is provided AS-IS.
  *
  ******************************************************************************
  */

/*
 * GIT_SHA         "27f5d5bcb9ca9522b73a54d7aec841664ee448cd"
 * GIT_BRANCH      "STAI-2.1"
 * GIT_DESCRIPTION "atonn-v1.1.0-31-g27f5d5bc"
 *
 * Command Line options:
 * --onnx-input = "/home/lukasschoepf/Documents/ProjectWork_2/Projectwork2_Code/ST32N6AI/image_classification/STM32N6/Model/ownModels/matmulModel_16_float/NetworkFiles/output/matmulModel_16_float_OE_3_2_0.onnx"
 * --out-dir-prefix = "/home/lukasschoepf/Documents/ProjectWork_2/Projectwork2_Code/ST32N6AI/image_classification/STM32N6/Model/ownModels/matmulModel_16_float/NetworkFiles/neural_art__network/"
 * --all-buffers-info = true
 * --mvei = true
 * --load-mdesc-file = "/opt/ST/STEdgeAI/2.1/Utilities/configs/stm32n6"
 * --load-mpool-file = "/home/lukasschoepf/Documents/ProjectWork_2/Projectwork2_Code/ST32N6AI/image_classification/STM32N6/Model/my_mpools/ramOnly_NUCLEO-N657X0-Q"
 * --cache-maintenance = true
 * --enable-virtual-mem-pools = true
 * --json-quant-file = "/home/lukasschoepf/Documents/ProjectWork_2/Projectwork2_Code/ST32N6AI/image_classification/STM32N6/Model/ownModels/matmulModel_16_float/NetworkFiles/output/matmulModel_16_float_OE_3_2_0_Q.json"
 * --optimization = 3
 * --Os = true
 * --Omax-ca-pipe = 4
 * --Ocache-opt = true
 * --output-info-file = "c_info"
 * --Oalt-sched = true
 */

#include "ll_aton_NN_interface.h"
#include "ll_aton.h"
#include "ll_aton_lib.h"
#include "ll_aton_version.h"
#include "ll_sw.h"
#include "ATON_MatMul.h"

#if LL_ATON_VERSION_MAJOR != 1 || LL_ATON_VERSION_MINOR != 1 || LL_ATON_VERSION_MICRO != 0 || LL_ATON_VERSION_DEV != 31
#  warning "Possible mismatch in ll_aton library used"
#endif

#if !defined(LL_ATON_DBG_BUFFER_INFO_EXCLUDED)
#  define LL_ATON_DBG_BUFFER_INFO_EXCLUDED 0
#endif

/* global pool 6 is 1.12 KB */
/* index=6 file postfix=AXISRAM3_AXISRAM4_AXISRAM5_AXISRAM6 name=npuRAM3_npuRAM4_npuRAM5_npuRAM6 offset=0x34200000  absolute_mode size=1835000 vpool READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=16.201 use4initializers=NO score=95  */
/* global pool 1 is ? */
/* index=1 file postfix=AXISRAM5 name=npuRAM5 offset=0x342e0000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 2 is ? */
/* index=2 file postfix=AXISRAM4 name=npuRAM4 offset=0x34270000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 3 is 1.12 KB */
/* index=3 file postfix=AXISRAM3 name=npuRAM3 offset=0x34200000  absolute_mode size=458752 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=18.531 write_power=16.201 use4initializers=NO score=94  */
/* global pool 0 is ? */
/* index=0 file postfix=AXISRAM6 name=npuRAM6 offset=0x34350000  absolute_mode size=458744 READ_WRITE THROUGHPUT=HIGH LATENCY=LOW byte width=8 freq ratio=1.25 burst max length=MAXINT burst penalty=0 pipelined=ON cacheable=OFF read_power=19.006 write_power=15.79 use4initializers=NO score=94  */

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Input_Buffer_Float(uint32_t num, void* buffer, uint32_t size)
{
  {
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Input_Buffer_Float(uint32_t num)
{
  {
    return NULL;
  }
}

LL_ATON_User_IO_Result_t LL_ATON_Set_User_Output_Buffer_Float(uint32_t num, void* buffer, uint32_t size)
{
  {
    return LL_ATON_User_IO_WRONG_INDEX;
  }
}

void *LL_ATON_Get_User_Output_Buffer_Float(uint32_t num)
{
  {
    return NULL;
  }
}

bool LL_ATON_EC_Network_Init_Float(void)
{
  return true;
}

bool LL_ATON_EC_Inference_Init_Float(void)
{
  return true;
}

/* scheduling epoch=0    nodes=2   ------------------------------------------------------------------- */

/* scheduling epoch=1    nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=2    nodes=1   ------------------------------------------------------------------- */

static void LL_ATON_End_EpochBlock_Float(const void *epoch_block)
{
  LL_ATON_LIB_UNUSED(epoch_block);
  extern volatile Matmul_info matmulInfo_Float;

	/* Unit= 27 [PROCESSOR 0] */
	/* kind=Conv node=Gemm_1_conv_4 */
  Conv_sw_info conv1_sw_info = {
    /* "general.input" tensor-related info: */
    .general.input.dim.tensor_b = 1,
    .general.input.dim.tensor_h = 1,
    .general.input.dim.tensor_w = 1,
    .general.input.dim.tensor_c = matmulInfo_Float.insize,
    .general.input.dim.num_elem = matmulInfo_Float.insize,
    .general.input.stride.b = matmulInfo_Float.insize * matmulInfo_Float.bytes,
    .general.input.stride.h = matmulInfo_Float.insize * matmulInfo_Float.bytes,
    .general.input.stride.w = matmulInfo_Float.insize * matmulInfo_Float.bytes,
    .general.input.stride.c = matmulInfo_Float.bytes,
    .general.input.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + matmulInfo_Float.input_start))) /* Equivalent hex address = 0x34200400UL */,
    .general.input.format.is_signed = 0,
    /* "weights" tensor-related info: */
    .weights.dim.tensor_b = matmulInfo_Float.outsize,
    .weights.dim.tensor_h = 1,
    .weights.dim.tensor_w = 1,
    .weights.dim.tensor_c = matmulInfo_Float.insize,
    .weights.dim.num_elem = matmulInfo_Float.insize * matmulInfo_Float.outsize,
    .weights.stride.b = matmulInfo_Float.insize * matmulInfo_Float.bytes,
    .weights.stride.h = matmulInfo_Float.insize * matmulInfo_Float.bytes,
    .weights.stride.w = matmulInfo_Float.insize * matmulInfo_Float.bytes,
    .weights.stride.c = matmulInfo_Float.bytes,
    .weights.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + matmulInfo_Float.weight_start))) /* Equivalent hex address = 0x34200000UL */,
    .weights.format.is_signed = 0,
    /* "general.output" tensor-related info: */
    .general.output.dim.tensor_b = 1,
    .general.output.dim.tensor_h = 1,
    .general.output.dim.tensor_w = 1,
    .general.output.dim.tensor_c = matmulInfo_Float.outsize,
    .general.output.dim.num_elem = matmulInfo_Float.outsize,
    .general.output.stride.b = matmulInfo_Float.outsize * matmulInfo_Float.bytes,
    .general.output.stride.h = matmulInfo_Float.outsize * matmulInfo_Float.bytes,
    .general.output.stride.w = matmulInfo_Float.outsize * matmulInfo_Float.bytes,
    .general.output.stride.c = matmulInfo_Float.bytes,
    .general.output.mem.start_offset = ((unsigned char *)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + matmulInfo_Float.output_start))) /* Equivalent hex address = 0x34200440UL */,
    .general.output.format.is_signed = 1,
    /* Node-specific Hyper-parameters: */
    .ngroup = 1,
    .pads = {0, 0, 0, 0},
    .strides = {1, 1},
    .dilations = {1, 1},
    .general.type = LL_SW_CONV,
  };

  /* Low Level SW Layer function invocation. This will exploit EmbedNets libs) */
  /* Node Gemm_1_conv_4 mapped on EmbedNets (FLOAT) as Conv | Category: Computational */
  ll_sw_forward_conv(&conv1_sw_info);
  /* *** MCU cache clean (only) operation (SW, whole range) *** */
  /*     memory pool: 6 */
  /*     start: ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 1088))) */
  /*     end:   ((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + 1152))) */
  LL_ATON_Cache_MCU_Clean_Range(((uintptr_t)(ATON_LIB_PHYSICAL_TO_VIRTUAL_ADDR(0x34200000UL + matmulInfo_Float.input_start))) /* Equivalent hex address = 0x34200440UL */, matmulInfo_Float.output_end - matmulInfo_Float.input_start);

}


/* scheduling epoch=3    nodes=1   ------------------------------------------------------------------- */

/* scheduling epoch=4    nodes=1   ------------------------------------------------------------------- */

/* scheduling DONE                 ------------------------------------------------------------------- */

const EpochBlock_ItemTypeDef *LL_ATON_EpochBlockItems_Float(void) {

  static const EpochBlock_ItemTypeDef ll_atonn_rt_epoch_block_array[] = {
    {
      .start_epoch_block = NULL,
      .end_epoch_block = LL_ATON_End_EpochBlock_Float,
      .wait_mask = 0x00000000,
      .flags = EpochBlock_Flags_epoch_start | EpochBlock_Flags_epoch_end | EpochBlock_Flags_pure_sw,
#ifdef LL_ATON_EB_DBG_INFO
      .epoch_num = 2,
      .last_epoch_num = 2,
      .in_streng_mask = 0x00000000,
      .out_streng_mask = 0x00000000,
      .estimated_npu_cycles = 0,
      .estimated_tot_cycles = 0,
#endif // LL_ATON_EB_DBG_INFO
    },
    {
      .flags = EpochBlock_Flags_last_eb,
    },
  };


  return ll_atonn_rt_epoch_block_array;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Input_Buffers_Info_Float(void)
{
	extern volatile Matmul_info matmulInfo_Float;
  static const uint32_t buff_info__shape_1_16[] = { 1, 1, 16, 1 };
  static const uint32_t buff_info__mem_shape_U_1_16[] = { 1, 16 };
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  static const uint32_t buff_info__shape_16_16_1_1[] = { 16, 1, 1, 16 };
  static const uint32_t buff_info__mem_shape_F_16_16_1_1[] = { 16, 16, 1, 1 };
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
  LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Input_0_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
	  .offset_start = matmulInfo_Float.input_start,
	  .offset_end = matmulInfo_Float.input_end,
	  .offset_limit = matmulInfo_Float.input_limit,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 0,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_16,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16,
    },
#if LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = "Gemm_1_weights_transposed_3",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 0,
      .offset_end = 1024,
      .offset_limit = 1088,
      .is_user_allocated = 0,
      .is_param = 1,
      .epoch = 0,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_F_16_16_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_16_16_1_1,
    },
#endif // LL_ATON_DBG_BUFFER_INFO_EXCLUDED == 0
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Output_Buffers_Info_Float(void)
{
	extern volatile Matmul_info matmulInfo_Float;
  static const uint32_t buff_info__shape_1_16[] = { 1, 1, 16, 1 };
  static const uint32_t buff_info__mem_shape_U_1_16[] = { 1, 16 };
  LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Gemm_1_out_0",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
	  .offset_start = matmulInfo_Float.output_start,
	   .offset_end = matmulInfo_Float.output_end,
	  .offset_limit = matmulInfo_Float.output_limit,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 3,
      .batch = 1,
      .mem_shape = buff_info__mem_shape_U_1_16,
      .mem_ndims = 2,
      .chpos = CHPos_UNDEFINED,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}

const LL_Buffer_InfoTypeDef *LL_ATON_Internal_Buffers_Info_Float(void)
{
  static const uint32_t buff_info__shape_1_16_1_1[] = { 1, 1, 1, 16 };
  static const uint32_t buff_info__mem_shape_F_1_16_1_1[] = { 1, 16, 1, 1 };
  static const LL_Buffer_InfoTypeDef buff_info[] = {
    {
      .name = "Gemm_1_reshape_x_2",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 1024,
      .offset_end = 1088,
      .offset_limit = 1152,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 1,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_F_1_16_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 0,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_1_1,
    },
    {
      .name = "Gemm_1_conv_4",
      .addr_base = {(unsigned char *)(0x34200000UL) /* Equivalent hex address = 0x34200000UL */},
      .offset_start = 1088,
      .offset_end = 1152,
      .offset_limit = 1216,
      .is_user_allocated = 0,
      .is_param = 0,
      .epoch = 2,
      .batch = 16,
      .mem_shape = buff_info__mem_shape_F_1_16_1_1,
      .mem_ndims = 4,
      .chpos = CHPos_First,
      .Qm = 0,
      .Qn = 0,
      .Qunsigned = 1,
      .type = DataType_FLOAT,
      .nbits = 32,
      .ndims = 4,
      .shape = buff_info__shape_1_16_1_1,
    },
    {
      .name = NULL,
    }
  };

  return buff_info;
}





